{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @Trump on Twitter and Financial Market Volatility\n",
    "In this assignment, we mine the tweets of the president of America - Donald Trump by sentiment analysis, and try to correlate it with the VIX index, also known as the *fear* index and capturing market volatility.\n",
    "\n",
    "Note: this notebook is modified from Tutorial 2 notebook. I rewrote a few inefficient loops for conciseness and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:07.683790Z",
     "start_time": "2020-01-24T01:30:05.968045Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.stats import t as t_dist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from matplotlib import style\n",
    "import topicmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:07.692433Z",
     "start_time": "2020-01-24T01:30:07.686792Z"
    }
   },
   "outputs": [],
   "source": [
    "style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trump's tweets are retrieved from [here](https://www.kaggle.com/jared4robertson/trumptweets). The original corpus contains large amount of URLs as pictures or hyperlinks in the tweets. We cleaned those URLs from the corpus. We encountered the same problem as the tutorial notebook: the text data appear to be encoded in non-UTF8 format. After some experiment, the text could be correctly decoded with `windows-1252`. It turned out that the FOMC files should be decoded with this format as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.036148Z",
     "start_time": "2020-01-24T01:30:07.695816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Trump's tweets\n",
    "tweets = pd.read_csv('TrumpTweets.csv', encoding='windows-1252', parse_dates=['created_at'])\n",
    "tweets = tweets.sort_values('created_at').dropna(subset=['created_at'])\n",
    "\n",
    "# Remove url\n",
    "tweets.text = tweets.text.map(lambda text: re.sub(r'http\\S+', '', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then individual tweets were grouped on a daily basis. All tweets in a single day will be joined together to form a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.299016Z",
     "start_time": "2020-01-24T01:30:11.038316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2009-05-04    [Be sure to tune in and watch Donald Trump on ...\n",
       "2009-05-05    [Donald Trump will be appearing on The View to...\n",
       "2009-05-08    [Donald Trump reads Top Ten Financial Tips on ...\n",
       "2009-05-12    [My persona will never be that of a wallflower...\n",
       "2009-05-13    [Listen to an interview with Donald Trump disc...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_tweets = tweets[['text', 'created_at']].groupby(pd.Grouper(key='created_at', freq='D')).apply(lambda x: x.text.values)\n",
    "daily_tweets = daily_tweets[daily_tweets.map(lambda x: len(x)) != 0]  # Remove days with no tweets\n",
    "daily_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the period covering Trump's presidency. According to [wikipedia](https://en.wikipedia.org/wiki/Presidency_of_Donald_Trump), his presidency started on Jan 20, 2017, and the last available tweet in the dataset is on Dec 05, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.315473Z",
     "start_time": "2020-01-24T01:30:11.302722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2017-01-20    [Thank you for joining us at the Lincoln Memor...\n",
       "2017-01-21    [THANK YOU for another wonderful evening in Wa...\n",
       "2017-01-22    [Had a great meeting at CIA Headquarters yeste...\n",
       "2017-01-23    [Busy week planned with a heavy focus on jobs ...\n",
       "2017-01-24    [Will be meeting at 9:00 with top automobile e...\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRUMP_PRESIDENCY_START = '2017-01-20'\n",
    "# Select only tweets in his presidency\n",
    "daily_tweets = daily_tweets.loc[TRUMP_PRESIDENCY_START:]\n",
    "daily_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topicmodels Package Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.334133Z",
     "start_time": "2020-01-24T01:30:11.318477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for joining us at the Lincoln Memorial tonight- a very special evening! Together we are go\n"
     ]
    }
   ],
   "source": [
    "texts=[]\n",
    "for d, twts in daily_tweets.iteritems():\n",
    "    text=' '.join(list(twts))\n",
    "    texts.append(text)\n",
    "\n",
    "#Show part of the corpus\n",
    "print((texts[0])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.822667Z",
     "start_time": "2020-01-24T01:30:11.342889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n",
      "['thank', 'you', 'for', 'joining', 'us', 'at', 'the', 'lincoln', 'memorial', 'tonight', '-', 'a', 'very', 'special', 'evening', '!', 'together', 'we', 'are', 'going', 'to', 'make', 'am', 'â€¦', 'thank', 'you', 'for', 'a', 'wonderful', 'evening', 'in', 'washington', 'd', '.', 'c', '.', '#', 'inauguration', 'it', 'all', 'begins', 'today', '!', 'i', 'will', 'see', 'you', 'at', '11', ':']\n"
     ]
    }
   ],
   "source": [
    "# creates object for pre-processing\n",
    "objs=topicmodels.RawDocs(texts,'long')\n",
    "# length of the example\n",
    "print(len(objs.tokens))\n",
    "# take the list and only show the first 50 tokens\n",
    "print((objs.tokens[0])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.917196Z",
     "start_time": "2020-01-24T01:30:11.829152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'another',\n",
       " 'wonderful',\n",
       " 'evening',\n",
       " 'in',\n",
       " 'washington',\n",
       " 'together',\n",
       " 'we',\n",
       " 'will',\n",
       " 'make',\n",
       " 'america',\n",
       " 'great',\n",
       " 'again',\n",
       " 'fantastic',\n",
       " 'day',\n",
       " 'and',\n",
       " 'evening',\n",
       " 'in',\n",
       " 'washington',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'to',\n",
       " 'foxnews',\n",
       " 'and',\n",
       " 'so',\n",
       " 'many',\n",
       " 'other',\n",
       " 'news']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs.token_clean(1)\n",
    "objs.tokens[1][:30] # first thirty tokens of second day's tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to keep track of the dimensionality of the data as we go through different pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.952558Z",
     "start_time": "2020-01-24T01:30:11.920655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens = 13713\n",
      "number of total tokens = 308165\n"
     ]
    }
   ],
   "source": [
    "all_stems = [s for d in objs.tokens for s in d]\n",
    "print('number of unique tokens = '+str(len(set(all_stems))))\n",
    "print('number of total tokens = '+str(len(all_stems)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in pre-processing is to remove stopwords, which here have been defined by the \"long\" argument to RawDocs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:11.963822Z",
     "start_time": "2020-01-24T01:30:11.955810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'few',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'get',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'high',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'least',\n",
       " 'less',\n",
       " 'like',\n",
       " 'long',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'never',\n",
       " 'new',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'put',\n",
       " 'said',\n",
       " 'same',\n",
       " 'say',\n",
       " 'says',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seen',\n",
       " 'she',\n",
       " 'should',\n",
       " 'since',\n",
       " 'so',\n",
       " 'some',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'three',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'two',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'us',\n",
       " 'very',\n",
       " 'was',\n",
       " 'way',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'with',\n",
       " 'would',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs.stopwords # the stopwords removed in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:12.033012Z",
     "start_time": "2020-01-24T01:30:11.966374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens = 13550\n",
      "number of total tokens = 167784\n"
     ]
    }
   ],
   "source": [
    "objs.stopword_remove(\"tokens\")\n",
    "all_stems = [s for d in objs.tokens for s in d]\n",
    "print('number of unique tokens = '+str(len(set(all_stems))))\n",
    "print('number of total tokens = '+str(len(all_stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:16.940756Z",
     "start_time": "2020-01-24T01:30:12.039516Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens = 13550\n",
      "number of total tokens = 167784\n"
     ]
    }
   ],
   "source": [
    "objs.stem()\n",
    "objs.stopword_remove(\"stems\") # remove stems that are on the stopword list\n",
    "all_stems = [s for d in objs.tokens for s in d]\n",
    "print('number of unique tokens = '+str(len(set(all_stems))))\n",
    "print('number of total tokens = '+str(len(all_stems)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Module Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:16.952108Z",
     "start_time": "2020-01-24T01:30:16.946609Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import the necessary NLTK module and sklearn module \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for preprocessing procedures. In this function, we should\n",
    "- remove the non-alphabetic tokens contents\n",
    "- tokenize the text\n",
    "- remove stopping words\n",
    "- stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:30:16.965136Z",
     "start_time": "2020-01-24T01:30:16.955951Z"
    }
   },
   "outputs": [],
   "source": [
    "def mypreprocess(text, porter=True):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r'[^a-z]+',' ',text)\n",
    "    #tokenize the words\n",
    "    token1=word_tokenize(text)\n",
    "    token2=[t for t in token1 if t not in stopwords.words('english')]\n",
    "    #stemming transformation\n",
    "    if porter:\n",
    "        token3=[PorterStemmer().stem(t) for t in token2]\n",
    "    else:\n",
    "        token3=[LancasterStemmer().stem(t) for t in token2]\n",
    "    return token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets on date 2017-01-20 00:00:00\n",
      "Processing tweets on date 2017-01-21 00:00:00\n",
      "Processing tweets on date 2017-01-22 00:00:00\n",
      "Processing tweets on date 2017-01-23 00:00:00\n",
      "Processing tweets on date 2017-01-24 00:00:00\n",
      "Processing tweets on date 2017-01-25 00:00:00\n",
      "Processing tweets on date 2017-01-26 00:00:00\n",
      "Processing tweets on date 2017-01-27 00:00:00\n",
      "Processing tweets on date 2017-01-28 00:00:00\n",
      "Processing tweets on date 2017-01-29 00:00:00\n",
      "Processing tweets on date 2017-01-30 00:00:00\n",
      "Processing tweets on date 2017-01-31 00:00:00\n",
      "Processing tweets on date 2017-02-01 00:00:00\n",
      "Processing tweets on date 2017-02-02 00:00:00\n",
      "Processing tweets on date 2017-02-03 00:00:00\n",
      "Processing tweets on date 2017-02-04 00:00:00\n",
      "Processing tweets on date 2017-02-05 00:00:00\n",
      "Processing tweets on date 2017-02-06 00:00:00\n",
      "Processing tweets on date 2017-02-07 00:00:00\n",
      "Processing tweets on date 2017-02-08 00:00:00\n",
      "Processing tweets on date 2017-02-09 00:00:00\n",
      "Processing tweets on date 2017-02-10 00:00:00\n",
      "Processing tweets on date 2017-02-11 00:00:00\n",
      "Processing tweets on date 2017-02-12 00:00:00\n",
      "Processing tweets on date 2017-02-13 00:00:00\n",
      "Processing tweets on date 2017-02-14 00:00:00\n",
      "Processing tweets on date 2017-02-15 00:00:00\n",
      "Processing tweets on date 2017-02-16 00:00:00\n",
      "Processing tweets on date 2017-02-17 00:00:00\n",
      "Processing tweets on date 2017-02-18 00:00:00\n",
      "Processing tweets on date 2017-02-19 00:00:00\n",
      "Processing tweets on date 2017-02-20 00:00:00\n",
      "Processing tweets on date 2017-02-21 00:00:00\n",
      "Processing tweets on date 2017-02-22 00:00:00\n",
      "Processing tweets on date 2017-02-23 00:00:00\n",
      "Processing tweets on date 2017-02-24 00:00:00\n",
      "Processing tweets on date 2017-02-25 00:00:00\n",
      "Processing tweets on date 2017-02-26 00:00:00\n",
      "Processing tweets on date 2017-02-27 00:00:00\n",
      "Processing tweets on date 2017-02-28 00:00:00\n",
      "Processing tweets on date 2017-03-01 00:00:00\n",
      "Processing tweets on date 2017-03-02 00:00:00\n",
      "Processing tweets on date 2017-03-03 00:00:00\n",
      "Processing tweets on date 2017-03-04 00:00:00\n",
      "Processing tweets on date 2017-03-05 00:00:00\n",
      "Processing tweets on date 2017-03-06 00:00:00\n",
      "Processing tweets on date 2017-03-07 00:00:00\n",
      "Processing tweets on date 2017-03-08 00:00:00\n",
      "Processing tweets on date 2017-03-09 00:00:00\n",
      "Processing tweets on date 2017-03-10 00:00:00\n",
      "Processing tweets on date 2017-03-11 00:00:00\n",
      "Processing tweets on date 2017-03-13 00:00:00\n",
      "Processing tweets on date 2017-03-14 00:00:00\n",
      "Processing tweets on date 2017-03-15 00:00:00\n",
      "Processing tweets on date 2017-03-16 00:00:00\n",
      "Processing tweets on date 2017-03-17 00:00:00\n",
      "Processing tweets on date 2017-03-18 00:00:00\n",
      "Processing tweets on date 2017-03-19 00:00:00\n",
      "Processing tweets on date 2017-03-20 00:00:00\n",
      "Processing tweets on date 2017-03-21 00:00:00\n",
      "Processing tweets on date 2017-03-22 00:00:00\n",
      "Processing tweets on date 2017-03-23 00:00:00\n",
      "Processing tweets on date 2017-03-24 00:00:00\n",
      "Processing tweets on date 2017-03-25 00:00:00\n",
      "Processing tweets on date 2017-03-26 00:00:00\n",
      "Processing tweets on date 2017-03-27 00:00:00\n",
      "Processing tweets on date 2017-03-28 00:00:00\n",
      "Processing tweets on date 2017-03-29 00:00:00\n",
      "Processing tweets on date 2017-03-30 00:00:00\n",
      "Processing tweets on date 2017-03-31 00:00:00\n",
      "Processing tweets on date 2017-04-01 00:00:00\n",
      "Processing tweets on date 2017-04-02 00:00:00\n",
      "Processing tweets on date 2017-04-03 00:00:00\n",
      "Processing tweets on date 2017-04-04 00:00:00\n",
      "Processing tweets on date 2017-04-05 00:00:00\n",
      "Processing tweets on date 2017-04-06 00:00:00\n",
      "Processing tweets on date 2017-04-08 00:00:00\n",
      "Processing tweets on date 2017-04-09 00:00:00\n",
      "Processing tweets on date 2017-04-10 00:00:00\n",
      "Processing tweets on date 2017-04-11 00:00:00\n",
      "Processing tweets on date 2017-04-12 00:00:00\n",
      "Processing tweets on date 2017-04-13 00:00:00\n",
      "Processing tweets on date 2017-04-14 00:00:00\n",
      "Processing tweets on date 2017-04-16 00:00:00\n",
      "Processing tweets on date 2017-04-17 00:00:00\n",
      "Processing tweets on date 2017-04-18 00:00:00\n",
      "Processing tweets on date 2017-04-19 00:00:00\n",
      "Processing tweets on date 2017-04-20 00:00:00\n",
      "Processing tweets on date 2017-04-21 00:00:00\n",
      "Processing tweets on date 2017-04-22 00:00:00\n",
      "Processing tweets on date 2017-04-23 00:00:00\n",
      "Processing tweets on date 2017-04-24 00:00:00\n",
      "Processing tweets on date 2017-04-25 00:00:00\n",
      "Processing tweets on date 2017-04-26 00:00:00\n",
      "Processing tweets on date 2017-04-27 00:00:00\n",
      "Processing tweets on date 2017-04-28 00:00:00\n",
      "Processing tweets on date 2017-04-29 00:00:00\n",
      "Processing tweets on date 2017-04-30 00:00:00\n",
      "Processing tweets on date 2017-05-01 00:00:00\n",
      "Processing tweets on date 2017-05-02 00:00:00\n",
      "Processing tweets on date 2017-05-03 00:00:00\n",
      "Processing tweets on date 2017-05-04 00:00:00\n",
      "Processing tweets on date 2017-05-05 00:00:00\n",
      "Processing tweets on date 2017-05-06 00:00:00\n",
      "Processing tweets on date 2017-05-07 00:00:00\n",
      "Processing tweets on date 2017-05-08 00:00:00\n",
      "Processing tweets on date 2017-05-09 00:00:00\n",
      "Processing tweets on date 2017-05-10 00:00:00\n",
      "Processing tweets on date 2017-05-11 00:00:00\n",
      "Processing tweets on date 2017-05-12 00:00:00\n",
      "Processing tweets on date 2017-05-13 00:00:00\n",
      "Processing tweets on date 2017-05-14 00:00:00\n",
      "Processing tweets on date 2017-05-15 00:00:00\n",
      "Processing tweets on date 2017-05-16 00:00:00\n",
      "Processing tweets on date 2017-05-17 00:00:00\n",
      "Processing tweets on date 2017-05-18 00:00:00\n",
      "Processing tweets on date 2017-05-19 00:00:00\n",
      "Processing tweets on date 2017-05-20 00:00:00\n",
      "Processing tweets on date 2017-05-21 00:00:00\n",
      "Processing tweets on date 2017-05-22 00:00:00\n",
      "Processing tweets on date 2017-05-23 00:00:00\n",
      "Processing tweets on date 2017-05-24 00:00:00\n",
      "Processing tweets on date 2017-05-25 00:00:00\n",
      "Processing tweets on date 2017-05-26 00:00:00\n",
      "Processing tweets on date 2017-05-27 00:00:00\n",
      "Processing tweets on date 2017-05-28 00:00:00\n",
      "Processing tweets on date 2017-05-29 00:00:00\n",
      "Processing tweets on date 2017-05-30 00:00:00\n",
      "Processing tweets on date 2017-05-31 00:00:00\n",
      "Processing tweets on date 2017-06-01 00:00:00\n",
      "Processing tweets on date 2017-06-02 00:00:00\n",
      "Processing tweets on date 2017-06-03 00:00:00\n",
      "Processing tweets on date 2017-06-04 00:00:00\n",
      "Processing tweets on date 2017-06-05 00:00:00\n",
      "Processing tweets on date 2017-06-06 00:00:00\n",
      "Processing tweets on date 2017-06-07 00:00:00\n",
      "Processing tweets on date 2017-06-09 00:00:00\n",
      "Processing tweets on date 2017-06-10 00:00:00\n",
      "Processing tweets on date 2017-06-11 00:00:00\n",
      "Processing tweets on date 2017-06-12 00:00:00\n",
      "Processing tweets on date 2017-06-13 00:00:00\n",
      "Processing tweets on date 2017-06-14 00:00:00\n",
      "Processing tweets on date 2017-06-15 00:00:00\n",
      "Processing tweets on date 2017-06-16 00:00:00\n",
      "Processing tweets on date 2017-06-17 00:00:00\n",
      "Processing tweets on date 2017-06-18 00:00:00\n",
      "Processing tweets on date 2017-06-19 00:00:00\n",
      "Processing tweets on date 2017-06-20 00:00:00\n",
      "Processing tweets on date 2017-06-21 00:00:00\n",
      "Processing tweets on date 2017-06-22 00:00:00\n",
      "Processing tweets on date 2017-06-23 00:00:00\n",
      "Processing tweets on date 2017-06-24 00:00:00\n",
      "Processing tweets on date 2017-06-25 00:00:00\n",
      "Processing tweets on date 2017-06-26 00:00:00\n",
      "Processing tweets on date 2017-06-27 00:00:00\n",
      "Processing tweets on date 2017-06-28 00:00:00\n",
      "Processing tweets on date 2017-06-29 00:00:00\n",
      "Processing tweets on date 2017-06-30 00:00:00\n",
      "Processing tweets on date 2017-07-01 00:00:00\n",
      "Processing tweets on date 2017-07-02 00:00:00\n",
      "Processing tweets on date 2017-07-03 00:00:00\n",
      "Processing tweets on date 2017-07-04 00:00:00\n",
      "Processing tweets on date 2017-07-05 00:00:00\n",
      "Processing tweets on date 2017-07-06 00:00:00\n",
      "Processing tweets on date 2017-07-07 00:00:00\n",
      "Processing tweets on date 2017-07-08 00:00:00\n",
      "Processing tweets on date 2017-07-09 00:00:00\n",
      "Processing tweets on date 2017-07-10 00:00:00\n",
      "Processing tweets on date 2017-07-11 00:00:00\n",
      "Processing tweets on date 2017-07-12 00:00:00\n",
      "Processing tweets on date 2017-07-13 00:00:00\n",
      "Processing tweets on date 2017-07-14 00:00:00\n",
      "Processing tweets on date 2017-07-15 00:00:00\n",
      "Processing tweets on date 2017-07-16 00:00:00\n",
      "Processing tweets on date 2017-07-17 00:00:00\n",
      "Processing tweets on date 2017-07-18 00:00:00\n",
      "Processing tweets on date 2017-07-19 00:00:00\n",
      "Processing tweets on date 2017-07-20 00:00:00\n",
      "Processing tweets on date 2017-07-21 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets on date 2017-07-22 00:00:00\n",
      "Processing tweets on date 2017-07-23 00:00:00\n",
      "Processing tweets on date 2017-07-24 00:00:00\n",
      "Processing tweets on date 2017-07-25 00:00:00\n",
      "Processing tweets on date 2017-07-26 00:00:00\n",
      "Processing tweets on date 2017-07-27 00:00:00\n",
      "Processing tweets on date 2017-07-28 00:00:00\n",
      "Processing tweets on date 2017-07-29 00:00:00\n",
      "Processing tweets on date 2017-07-30 00:00:00\n",
      "Processing tweets on date 2017-07-31 00:00:00\n",
      "Processing tweets on date 2017-08-01 00:00:00\n",
      "Processing tweets on date 2017-08-02 00:00:00\n",
      "Processing tweets on date 2017-08-03 00:00:00\n",
      "Processing tweets on date 2017-08-04 00:00:00\n",
      "Processing tweets on date 2017-08-05 00:00:00\n",
      "Processing tweets on date 2017-08-06 00:00:00\n",
      "Processing tweets on date 2017-08-07 00:00:00\n",
      "Processing tweets on date 2017-08-08 00:00:00\n",
      "Processing tweets on date 2017-08-09 00:00:00\n",
      "Processing tweets on date 2017-08-10 00:00:00\n",
      "Processing tweets on date 2017-08-11 00:00:00\n",
      "Processing tweets on date 2017-08-12 00:00:00\n",
      "Processing tweets on date 2017-08-14 00:00:00\n",
      "Processing tweets on date 2017-08-15 00:00:00\n",
      "Processing tweets on date 2017-08-16 00:00:00\n",
      "Processing tweets on date 2017-08-17 00:00:00\n",
      "Processing tweets on date 2017-08-18 00:00:00\n",
      "Processing tweets on date 2017-08-19 00:00:00\n",
      "Processing tweets on date 2017-08-20 00:00:00\n",
      "Processing tweets on date 2017-08-21 00:00:00\n",
      "Processing tweets on date 2017-08-22 00:00:00\n",
      "Processing tweets on date 2017-08-23 00:00:00\n",
      "Processing tweets on date 2017-08-24 00:00:00\n",
      "Processing tweets on date 2017-08-25 00:00:00\n",
      "Processing tweets on date 2017-08-26 00:00:00\n",
      "Processing tweets on date 2017-08-27 00:00:00\n",
      "Processing tweets on date 2017-08-29 00:00:00\n",
      "Processing tweets on date 2017-08-30 00:00:00\n",
      "Processing tweets on date 2017-08-31 00:00:00\n",
      "Processing tweets on date 2017-09-01 00:00:00\n",
      "Processing tweets on date 2017-09-02 00:00:00\n",
      "Processing tweets on date 2017-09-03 00:00:00\n",
      "Processing tweets on date 2017-09-04 00:00:00\n",
      "Processing tweets on date 2017-09-05 00:00:00\n",
      "Processing tweets on date 2017-09-06 00:00:00\n",
      "Processing tweets on date 2017-09-07 00:00:00\n",
      "Processing tweets on date 2017-09-08 00:00:00\n",
      "Processing tweets on date 2017-09-09 00:00:00\n",
      "Processing tweets on date 2017-09-10 00:00:00\n",
      "Processing tweets on date 2017-09-11 00:00:00\n",
      "Processing tweets on date 2017-09-12 00:00:00\n",
      "Processing tweets on date 2017-09-13 00:00:00\n",
      "Processing tweets on date 2017-09-14 00:00:00\n",
      "Processing tweets on date 2017-09-15 00:00:00\n",
      "Processing tweets on date 2017-09-16 00:00:00\n",
      "Processing tweets on date 2017-09-17 00:00:00\n",
      "Processing tweets on date 2017-09-18 00:00:00\n",
      "Processing tweets on date 2017-09-19 00:00:00\n",
      "Processing tweets on date 2017-09-20 00:00:00\n",
      "Processing tweets on date 2017-09-21 00:00:00\n",
      "Processing tweets on date 2017-09-22 00:00:00\n",
      "Processing tweets on date 2017-09-23 00:00:00\n",
      "Processing tweets on date 2017-09-24 00:00:00\n",
      "Processing tweets on date 2017-09-25 00:00:00\n",
      "Processing tweets on date 2017-09-26 00:00:00\n",
      "Processing tweets on date 2017-09-27 00:00:00\n",
      "Processing tweets on date 2017-09-28 00:00:00\n",
      "Processing tweets on date 2017-09-29 00:00:00\n",
      "Processing tweets on date 2017-09-30 00:00:00\n",
      "Processing tweets on date 2017-10-01 00:00:00\n",
      "Processing tweets on date 2017-10-02 00:00:00\n",
      "Processing tweets on date 2017-10-03 00:00:00\n",
      "Processing tweets on date 2017-10-04 00:00:00\n",
      "Processing tweets on date 2017-10-05 00:00:00\n",
      "Processing tweets on date 2017-10-06 00:00:00\n",
      "Processing tweets on date 2017-10-07 00:00:00\n",
      "Processing tweets on date 2017-10-08 00:00:00\n",
      "Processing tweets on date 2017-10-09 00:00:00\n",
      "Processing tweets on date 2017-10-10 00:00:00\n",
      "Processing tweets on date 2017-10-11 00:00:00\n",
      "Processing tweets on date 2017-10-12 00:00:00\n",
      "Processing tweets on date 2017-10-13 00:00:00\n",
      "Processing tweets on date 2017-10-14 00:00:00\n",
      "Processing tweets on date 2017-10-15 00:00:00\n",
      "Processing tweets on date 2017-10-16 00:00:00\n",
      "Processing tweets on date 2017-10-17 00:00:00\n",
      "Processing tweets on date 2017-10-18 00:00:00\n",
      "Processing tweets on date 2017-10-19 00:00:00\n",
      "Processing tweets on date 2017-10-20 00:00:00\n",
      "Processing tweets on date 2017-10-21 00:00:00\n",
      "Processing tweets on date 2017-10-22 00:00:00\n",
      "Processing tweets on date 2017-10-23 00:00:00\n",
      "Processing tweets on date 2017-10-24 00:00:00\n",
      "Processing tweets on date 2017-10-25 00:00:00\n",
      "Processing tweets on date 2017-10-26 00:00:00\n",
      "Processing tweets on date 2017-10-27 00:00:00\n",
      "Processing tweets on date 2017-10-28 00:00:00\n",
      "Processing tweets on date 2017-10-29 00:00:00\n",
      "Processing tweets on date 2017-10-30 00:00:00\n",
      "Processing tweets on date 2017-10-31 00:00:00\n",
      "Processing tweets on date 2017-11-01 00:00:00\n",
      "Processing tweets on date 2017-11-02 00:00:00\n",
      "Processing tweets on date 2017-11-03 00:00:00\n",
      "Processing tweets on date 2017-11-04 00:00:00\n",
      "Processing tweets on date 2017-11-05 00:00:00\n",
      "Processing tweets on date 2017-11-06 00:00:00\n",
      "Processing tweets on date 2017-11-07 00:00:00\n",
      "Processing tweets on date 2017-11-08 00:00:00\n",
      "Processing tweets on date 2017-11-09 00:00:00\n",
      "Processing tweets on date 2017-11-10 00:00:00\n",
      "Processing tweets on date 2017-11-11 00:00:00\n",
      "Processing tweets on date 2017-11-12 00:00:00\n",
      "Processing tweets on date 2017-11-13 00:00:00\n",
      "Processing tweets on date 2017-11-14 00:00:00\n",
      "Processing tweets on date 2017-11-15 00:00:00\n",
      "Processing tweets on date 2017-11-16 00:00:00\n",
      "Processing tweets on date 2017-11-17 00:00:00\n",
      "Processing tweets on date 2017-11-18 00:00:00\n",
      "Processing tweets on date 2017-11-19 00:00:00\n",
      "Processing tweets on date 2017-11-20 00:00:00\n",
      "Processing tweets on date 2017-11-21 00:00:00\n",
      "Processing tweets on date 2017-11-22 00:00:00\n",
      "Processing tweets on date 2017-11-23 00:00:00\n",
      "Processing tweets on date 2017-11-24 00:00:00\n",
      "Processing tweets on date 2017-11-25 00:00:00\n",
      "Processing tweets on date 2017-11-26 00:00:00\n",
      "Processing tweets on date 2017-11-27 00:00:00\n",
      "Processing tweets on date 2017-11-28 00:00:00\n",
      "Processing tweets on date 2017-11-29 00:00:00\n",
      "Processing tweets on date 2017-11-30 00:00:00\n",
      "Processing tweets on date 2017-12-01 00:00:00\n",
      "Processing tweets on date 2017-12-02 00:00:00\n",
      "Processing tweets on date 2017-12-03 00:00:00\n",
      "Processing tweets on date 2017-12-04 00:00:00\n",
      "Processing tweets on date 2017-12-05 00:00:00\n",
      "Processing tweets on date 2017-12-06 00:00:00\n",
      "Processing tweets on date 2017-12-07 00:00:00\n",
      "Processing tweets on date 2017-12-08 00:00:00\n",
      "Processing tweets on date 2017-12-09 00:00:00\n",
      "Processing tweets on date 2017-12-10 00:00:00\n",
      "Processing tweets on date 2017-12-11 00:00:00\n",
      "Processing tweets on date 2017-12-12 00:00:00\n",
      "Processing tweets on date 2017-12-13 00:00:00\n",
      "Processing tweets on date 2017-12-14 00:00:00\n",
      "Processing tweets on date 2017-12-15 00:00:00\n",
      "Processing tweets on date 2017-12-16 00:00:00\n",
      "Processing tweets on date 2017-12-17 00:00:00\n",
      "Processing tweets on date 2017-12-18 00:00:00\n",
      "Processing tweets on date 2017-12-19 00:00:00\n",
      "Processing tweets on date 2017-12-20 00:00:00\n",
      "Processing tweets on date 2017-12-21 00:00:00\n",
      "Processing tweets on date 2017-12-22 00:00:00\n",
      "Processing tweets on date 2017-12-23 00:00:00\n",
      "Processing tweets on date 2017-12-24 00:00:00\n",
      "Processing tweets on date 2017-12-25 00:00:00\n",
      "Processing tweets on date 2017-12-26 00:00:00\n",
      "Processing tweets on date 2017-12-27 00:00:00\n",
      "Processing tweets on date 2017-12-28 00:00:00\n",
      "Processing tweets on date 2017-12-29 00:00:00\n",
      "Processing tweets on date 2017-12-30 00:00:00\n",
      "Processing tweets on date 2017-12-31 00:00:00\n",
      "Processing tweets on date 2018-01-01 00:00:00\n",
      "Processing tweets on date 2018-01-02 00:00:00\n",
      "Processing tweets on date 2018-01-03 00:00:00\n",
      "Processing tweets on date 2018-01-04 00:00:00\n",
      "Processing tweets on date 2018-01-05 00:00:00\n",
      "Processing tweets on date 2018-01-06 00:00:00\n",
      "Processing tweets on date 2018-01-07 00:00:00\n",
      "Processing tweets on date 2018-01-08 00:00:00\n",
      "Processing tweets on date 2018-01-09 00:00:00\n",
      "Processing tweets on date 2018-01-10 00:00:00\n",
      "Processing tweets on date 2018-01-11 00:00:00\n",
      "Processing tweets on date 2018-01-12 00:00:00\n",
      "Processing tweets on date 2018-01-13 00:00:00\n",
      "Processing tweets on date 2018-01-14 00:00:00\n",
      "Processing tweets on date 2018-01-15 00:00:00\n",
      "Processing tweets on date 2018-01-16 00:00:00\n",
      "Processing tweets on date 2018-01-17 00:00:00\n",
      "Processing tweets on date 2018-01-18 00:00:00\n",
      "Processing tweets on date 2018-01-19 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets on date 2018-01-20 00:00:00\n",
      "Processing tweets on date 2018-01-21 00:00:00\n",
      "Processing tweets on date 2018-01-22 00:00:00\n",
      "Processing tweets on date 2018-01-23 00:00:00\n",
      "Processing tweets on date 2018-01-24 00:00:00\n",
      "Processing tweets on date 2018-01-25 00:00:00\n",
      "Processing tweets on date 2018-01-26 00:00:00\n",
      "Processing tweets on date 2018-01-27 00:00:00\n",
      "Processing tweets on date 2018-01-28 00:00:00\n",
      "Processing tweets on date 2018-01-29 00:00:00\n",
      "Processing tweets on date 2018-01-31 00:00:00\n",
      "Processing tweets on date 2018-02-01 00:00:00\n",
      "Processing tweets on date 2018-02-02 00:00:00\n",
      "Processing tweets on date 2018-02-03 00:00:00\n",
      "Processing tweets on date 2018-02-04 00:00:00\n",
      "Processing tweets on date 2018-02-05 00:00:00\n",
      "Processing tweets on date 2018-02-06 00:00:00\n",
      "Processing tweets on date 2018-02-07 00:00:00\n",
      "Processing tweets on date 2018-02-08 00:00:00\n",
      "Processing tweets on date 2018-02-09 00:00:00\n",
      "Processing tweets on date 2018-02-10 00:00:00\n",
      "Processing tweets on date 2018-02-11 00:00:00\n",
      "Processing tweets on date 2018-02-12 00:00:00\n",
      "Processing tweets on date 2018-02-13 00:00:00\n",
      "Processing tweets on date 2018-02-14 00:00:00\n",
      "Processing tweets on date 2018-02-15 00:00:00\n",
      "Processing tweets on date 2018-02-16 00:00:00\n",
      "Processing tweets on date 2018-02-17 00:00:00\n",
      "Processing tweets on date 2018-02-18 00:00:00\n",
      "Processing tweets on date 2018-02-19 00:00:00\n",
      "Processing tweets on date 2018-02-20 00:00:00\n",
      "Processing tweets on date 2018-02-21 00:00:00\n",
      "Processing tweets on date 2018-02-22 00:00:00\n",
      "Processing tweets on date 2018-02-23 00:00:00\n",
      "Processing tweets on date 2018-02-24 00:00:00\n",
      "Processing tweets on date 2018-02-25 00:00:00\n",
      "Processing tweets on date 2018-02-27 00:00:00\n",
      "Processing tweets on date 2018-02-28 00:00:00\n",
      "Processing tweets on date 2018-03-01 00:00:00\n",
      "Processing tweets on date 2018-03-02 00:00:00\n",
      "Processing tweets on date 2018-03-03 00:00:00\n",
      "Processing tweets on date 2018-03-04 00:00:00\n",
      "Processing tweets on date 2018-03-05 00:00:00\n",
      "Processing tweets on date 2018-03-06 00:00:00\n",
      "Processing tweets on date 2018-03-07 00:00:00\n",
      "Processing tweets on date 2018-03-08 00:00:00\n",
      "Processing tweets on date 2018-03-09 00:00:00\n",
      "Processing tweets on date 2018-03-10 00:00:00\n",
      "Processing tweets on date 2018-03-11 00:00:00\n",
      "Processing tweets on date 2018-03-12 00:00:00\n",
      "Processing tweets on date 2018-03-13 00:00:00\n",
      "Processing tweets on date 2018-03-14 00:00:00\n",
      "Processing tweets on date 2018-03-15 00:00:00\n",
      "Processing tweets on date 2018-03-16 00:00:00\n",
      "Processing tweets on date 2018-03-17 00:00:00\n",
      "Processing tweets on date 2018-03-18 00:00:00\n",
      "Processing tweets on date 2018-03-19 00:00:00\n",
      "Processing tweets on date 2018-03-20 00:00:00\n",
      "Processing tweets on date 2018-03-21 00:00:00\n",
      "Processing tweets on date 2018-03-22 00:00:00\n",
      "Processing tweets on date 2018-03-23 00:00:00\n",
      "Processing tweets on date 2018-03-24 00:00:00\n",
      "Processing tweets on date 2018-03-25 00:00:00\n",
      "Processing tweets on date 2018-03-26 00:00:00\n",
      "Processing tweets on date 2018-03-27 00:00:00\n",
      "Processing tweets on date 2018-03-28 00:00:00\n",
      "Processing tweets on date 2018-03-29 00:00:00\n",
      "Processing tweets on date 2018-03-30 00:00:00\n",
      "Processing tweets on date 2018-03-31 00:00:00\n",
      "Processing tweets on date 2018-04-01 00:00:00\n",
      "Processing tweets on date 2018-04-02 00:00:00\n",
      "Processing tweets on date 2018-04-03 00:00:00\n",
      "Processing tweets on date 2018-04-04 00:00:00\n",
      "Processing tweets on date 2018-04-05 00:00:00\n",
      "Processing tweets on date 2018-04-06 00:00:00\n",
      "Processing tweets on date 2018-04-07 00:00:00\n",
      "Processing tweets on date 2018-04-08 00:00:00\n",
      "Processing tweets on date 2018-04-09 00:00:00\n",
      "Processing tweets on date 2018-04-10 00:00:00\n",
      "Processing tweets on date 2018-04-11 00:00:00\n",
      "Processing tweets on date 2018-04-12 00:00:00\n",
      "Processing tweets on date 2018-04-13 00:00:00\n",
      "Processing tweets on date 2018-04-14 00:00:00\n",
      "Processing tweets on date 2018-04-15 00:00:00\n",
      "Processing tweets on date 2018-04-16 00:00:00\n",
      "Processing tweets on date 2018-04-17 00:00:00\n",
      "Processing tweets on date 2018-04-18 00:00:00\n",
      "Processing tweets on date 2018-04-19 00:00:00\n",
      "Processing tweets on date 2018-04-20 00:00:00\n",
      "Processing tweets on date 2018-04-21 00:00:00\n",
      "Processing tweets on date 2018-04-22 00:00:00\n",
      "Processing tweets on date 2018-04-23 00:00:00\n",
      "Processing tweets on date 2018-04-24 00:00:00\n",
      "Processing tweets on date 2018-04-25 00:00:00\n",
      "Processing tweets on date 2018-04-26 00:00:00\n",
      "Processing tweets on date 2018-04-27 00:00:00\n",
      "Processing tweets on date 2018-04-28 00:00:00\n",
      "Processing tweets on date 2018-04-29 00:00:00\n",
      "Processing tweets on date 2018-04-30 00:00:00\n",
      "Processing tweets on date 2018-05-01 00:00:00\n",
      "Processing tweets on date 2018-05-02 00:00:00\n",
      "Processing tweets on date 2018-05-03 00:00:00\n",
      "Processing tweets on date 2018-05-04 00:00:00\n",
      "Processing tweets on date 2018-05-05 00:00:00\n",
      "Processing tweets on date 2018-05-07 00:00:00\n",
      "Processing tweets on date 2018-05-08 00:00:00\n",
      "Processing tweets on date 2018-05-09 00:00:00\n",
      "Processing tweets on date 2018-05-10 00:00:00\n",
      "Processing tweets on date 2018-05-11 00:00:00\n",
      "Processing tweets on date 2018-05-12 00:00:00\n",
      "Processing tweets on date 2018-05-13 00:00:00\n",
      "Processing tweets on date 2018-05-14 00:00:00\n",
      "Processing tweets on date 2018-05-15 00:00:00\n",
      "Processing tweets on date 2018-05-16 00:00:00\n",
      "Processing tweets on date 2018-05-17 00:00:00\n",
      "Processing tweets on date 2018-05-18 00:00:00\n",
      "Processing tweets on date 2018-05-19 00:00:00\n",
      "Processing tweets on date 2018-05-20 00:00:00\n",
      "Processing tweets on date 2018-05-21 00:00:00\n",
      "Processing tweets on date 2018-05-22 00:00:00\n",
      "Processing tweets on date 2018-05-23 00:00:00\n",
      "Processing tweets on date 2018-05-24 00:00:00\n",
      "Processing tweets on date 2018-06-21 00:00:00\n",
      "Processing tweets on date 2018-06-22 00:00:00\n",
      "Processing tweets on date 2018-06-23 00:00:00\n",
      "Processing tweets on date 2018-06-24 00:00:00\n",
      "Processing tweets on date 2018-06-25 00:00:00\n",
      "Processing tweets on date 2018-06-26 00:00:00\n",
      "Processing tweets on date 2018-06-27 00:00:00\n",
      "Processing tweets on date 2018-06-28 00:00:00\n",
      "Processing tweets on date 2018-06-29 00:00:00\n",
      "Processing tweets on date 2018-06-30 00:00:00\n",
      "Processing tweets on date 2018-07-01 00:00:00\n",
      "Processing tweets on date 2018-07-02 00:00:00\n",
      "Processing tweets on date 2018-07-03 00:00:00\n",
      "Processing tweets on date 2018-07-04 00:00:00\n",
      "Processing tweets on date 2018-07-05 00:00:00\n",
      "Processing tweets on date 2018-07-06 00:00:00\n",
      "Processing tweets on date 2018-07-07 00:00:00\n",
      "Processing tweets on date 2018-07-08 00:00:00\n",
      "Processing tweets on date 2018-07-09 00:00:00\n",
      "Processing tweets on date 2018-07-10 00:00:00\n",
      "Processing tweets on date 2018-07-11 00:00:00\n",
      "Processing tweets on date 2018-07-12 00:00:00\n",
      "Processing tweets on date 2018-07-13 00:00:00\n",
      "Processing tweets on date 2018-07-14 00:00:00\n",
      "Processing tweets on date 2018-07-15 00:00:00\n",
      "Processing tweets on date 2018-07-16 00:00:00\n",
      "Processing tweets on date 2018-07-17 00:00:00\n",
      "Processing tweets on date 2018-07-18 00:00:00\n",
      "Processing tweets on date 2018-07-19 00:00:00\n",
      "Processing tweets on date 2018-07-20 00:00:00\n",
      "Processing tweets on date 2018-07-21 00:00:00\n",
      "Processing tweets on date 2018-07-22 00:00:00\n",
      "Processing tweets on date 2018-07-23 00:00:00\n",
      "Processing tweets on date 2018-07-24 00:00:00\n",
      "Processing tweets on date 2018-07-25 00:00:00\n",
      "Processing tweets on date 2018-07-26 00:00:00\n",
      "Processing tweets on date 2018-07-27 00:00:00\n",
      "Processing tweets on date 2018-07-28 00:00:00\n",
      "Processing tweets on date 2018-07-29 00:00:00\n",
      "Processing tweets on date 2018-07-30 00:00:00\n",
      "Processing tweets on date 2018-07-31 00:00:00\n",
      "Processing tweets on date 2018-08-01 00:00:00\n",
      "Processing tweets on date 2018-08-02 00:00:00\n",
      "Processing tweets on date 2018-08-03 00:00:00\n",
      "Processing tweets on date 2018-08-04 00:00:00\n",
      "Processing tweets on date 2018-08-05 00:00:00\n",
      "Processing tweets on date 2018-08-06 00:00:00\n",
      "Processing tweets on date 2018-08-07 00:00:00\n",
      "Processing tweets on date 2018-08-08 00:00:00\n",
      "Processing tweets on date 2018-08-09 00:00:00\n",
      "Processing tweets on date 2018-08-10 00:00:00\n",
      "Processing tweets on date 2018-08-11 00:00:00\n",
      "Processing tweets on date 2018-08-12 00:00:00\n",
      "Processing tweets on date 2018-08-13 00:00:00\n",
      "Processing tweets on date 2018-08-14 00:00:00\n",
      "Processing tweets on date 2018-08-15 00:00:00\n",
      "Processing tweets on date 2018-08-16 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets on date 2018-08-17 00:00:00\n",
      "Processing tweets on date 2018-08-18 00:00:00\n",
      "Processing tweets on date 2018-08-19 00:00:00\n",
      "Processing tweets on date 2018-08-20 00:00:00\n",
      "Processing tweets on date 2018-08-21 00:00:00\n",
      "Processing tweets on date 2018-08-22 00:00:00\n",
      "Processing tweets on date 2018-08-23 00:00:00\n",
      "Processing tweets on date 2018-08-24 00:00:00\n",
      "Processing tweets on date 2018-08-25 00:00:00\n",
      "Processing tweets on date 2018-08-26 00:00:00\n",
      "Processing tweets on date 2018-08-27 00:00:00\n",
      "Processing tweets on date 2018-08-28 00:00:00\n",
      "Processing tweets on date 2018-08-29 00:00:00\n",
      "Processing tweets on date 2018-08-30 00:00:00\n",
      "Processing tweets on date 2018-08-31 00:00:00\n",
      "Processing tweets on date 2018-09-01 00:00:00\n",
      "Processing tweets on date 2018-09-02 00:00:00\n",
      "Processing tweets on date 2018-09-03 00:00:00\n",
      "Processing tweets on date 2018-09-04 00:00:00\n",
      "Processing tweets on date 2018-09-05 00:00:00\n",
      "Processing tweets on date 2018-09-06 00:00:00\n",
      "Processing tweets on date 2018-09-07 00:00:00\n",
      "Processing tweets on date 2018-09-08 00:00:00\n",
      "Processing tweets on date 2018-09-09 00:00:00\n",
      "Processing tweets on date 2018-09-10 00:00:00\n",
      "Processing tweets on date 2018-09-11 00:00:00\n",
      "Processing tweets on date 2018-09-12 00:00:00\n",
      "Processing tweets on date 2018-09-13 00:00:00\n",
      "Processing tweets on date 2018-09-14 00:00:00\n",
      "Processing tweets on date 2018-09-15 00:00:00\n",
      "Processing tweets on date 2018-09-16 00:00:00\n",
      "Processing tweets on date 2018-09-17 00:00:00\n",
      "Processing tweets on date 2018-09-18 00:00:00\n",
      "Processing tweets on date 2018-09-19 00:00:00\n",
      "Processing tweets on date 2018-09-20 00:00:00\n",
      "Processing tweets on date 2018-09-21 00:00:00\n",
      "Processing tweets on date 2018-09-22 00:00:00\n",
      "Processing tweets on date 2018-09-23 00:00:00\n",
      "Processing tweets on date 2018-09-24 00:00:00\n",
      "Processing tweets on date 2018-09-25 00:00:00\n",
      "Processing tweets on date 2018-09-26 00:00:00\n",
      "Processing tweets on date 2018-09-27 00:00:00\n",
      "Processing tweets on date 2018-09-29 00:00:00\n",
      "Processing tweets on date 2018-09-30 00:00:00\n",
      "Processing tweets on date 2018-10-01 00:00:00\n",
      "Processing tweets on date 2018-10-02 00:00:00\n",
      "Processing tweets on date 2018-10-03 00:00:00\n",
      "Processing tweets on date 2018-10-04 00:00:00\n",
      "Processing tweets on date 2018-10-05 00:00:00\n",
      "Processing tweets on date 2018-10-06 00:00:00\n",
      "Processing tweets on date 2018-10-07 00:00:00\n",
      "Processing tweets on date 2018-10-08 00:00:00\n",
      "Processing tweets on date 2018-10-09 00:00:00\n",
      "Processing tweets on date 2018-10-10 00:00:00\n",
      "Processing tweets on date 2018-10-11 00:00:00\n",
      "Processing tweets on date 2018-10-12 00:00:00\n",
      "Processing tweets on date 2018-10-13 00:00:00\n",
      "Processing tweets on date 2018-10-14 00:00:00\n",
      "Processing tweets on date 2018-10-15 00:00:00\n",
      "Processing tweets on date 2018-10-16 00:00:00\n",
      "Processing tweets on date 2018-10-17 00:00:00\n",
      "Processing tweets on date 2018-10-18 00:00:00\n",
      "Processing tweets on date 2018-10-19 00:00:00\n",
      "Processing tweets on date 2018-10-20 00:00:00\n",
      "Processing tweets on date 2018-10-21 00:00:00\n",
      "Processing tweets on date 2018-10-22 00:00:00\n",
      "Processing tweets on date 2018-10-23 00:00:00\n",
      "Processing tweets on date 2018-10-24 00:00:00\n",
      "Processing tweets on date 2018-10-25 00:00:00\n",
      "Processing tweets on date 2018-10-26 00:00:00\n",
      "Processing tweets on date 2018-10-27 00:00:00\n",
      "Processing tweets on date 2018-10-28 00:00:00\n",
      "Processing tweets on date 2018-10-29 00:00:00\n",
      "Processing tweets on date 2018-10-30 00:00:00\n",
      "Processing tweets on date 2018-10-31 00:00:00\n",
      "Processing tweets on date 2018-11-01 00:00:00\n",
      "Processing tweets on date 2018-11-02 00:00:00\n",
      "Processing tweets on date 2018-11-03 00:00:00\n",
      "Processing tweets on date 2018-11-04 00:00:00\n",
      "Processing tweets on date 2018-11-05 00:00:00\n",
      "Processing tweets on date 2018-11-06 00:00:00\n",
      "Processing tweets on date 2018-11-07 00:00:00\n",
      "Processing tweets on date 2018-11-08 00:00:00\n",
      "Processing tweets on date 2018-11-09 00:00:00\n",
      "Processing tweets on date 2018-11-10 00:00:00\n",
      "Processing tweets on date 2018-11-11 00:00:00\n",
      "Processing tweets on date 2018-11-12 00:00:00\n",
      "Processing tweets on date 2018-11-13 00:00:00\n",
      "Processing tweets on date 2018-11-14 00:00:00\n",
      "Processing tweets on date 2018-11-15 00:00:00\n",
      "Processing tweets on date 2018-11-16 00:00:00\n",
      "Processing tweets on date 2018-11-17 00:00:00\n",
      "Processing tweets on date 2018-11-18 00:00:00\n",
      "Processing tweets on date 2018-11-19 00:00:00\n",
      "Processing tweets on date 2018-11-20 00:00:00\n",
      "Processing tweets on date 2018-11-21 00:00:00\n",
      "Processing tweets on date 2018-11-22 00:00:00\n",
      "Processing tweets on date 2018-11-23 00:00:00\n",
      "Processing tweets on date 2018-11-24 00:00:00\n",
      "Processing tweets on date 2018-11-25 00:00:00\n",
      "Processing tweets on date 2018-11-26 00:00:00\n",
      "Processing tweets on date 2018-11-27 00:00:00\n",
      "Processing tweets on date 2018-11-28 00:00:00\n",
      "Processing tweets on date 2018-11-29 00:00:00\n",
      "Processing tweets on date 2018-11-30 00:00:00\n",
      "Processing tweets on date 2018-12-01 00:00:00\n",
      "Processing tweets on date 2018-12-02 00:00:00\n",
      "Processing tweets on date 2018-12-03 00:00:00\n",
      "Processing tweets on date 2018-12-04 00:00:00\n",
      "Processing tweets on date 2018-12-05 00:00:00\n",
      "Processing tweets on date 2018-12-06 00:00:00\n",
      "Processing tweets on date 2018-12-07 00:00:00\n",
      "Processing tweets on date 2018-12-08 00:00:00\n",
      "Processing tweets on date 2018-12-09 00:00:00\n",
      "Processing tweets on date 2018-12-10 00:00:00\n",
      "Processing tweets on date 2018-12-11 00:00:00\n",
      "Processing tweets on date 2018-12-12 00:00:00\n",
      "Processing tweets on date 2018-12-13 00:00:00\n",
      "Processing tweets on date 2018-12-14 00:00:00\n",
      "Processing tweets on date 2018-12-15 00:00:00\n",
      "Processing tweets on date 2018-12-16 00:00:00\n",
      "Processing tweets on date 2018-12-17 00:00:00\n",
      "Processing tweets on date 2018-12-18 00:00:00\n",
      "Processing tweets on date 2018-12-19 00:00:00\n",
      "Processing tweets on date 2018-12-20 00:00:00\n",
      "Processing tweets on date 2018-12-21 00:00:00\n",
      "Processing tweets on date 2018-12-22 00:00:00\n",
      "Processing tweets on date 2018-12-23 00:00:00\n",
      "Processing tweets on date 2018-12-24 00:00:00\n",
      "Processing tweets on date 2018-12-25 00:00:00\n",
      "Processing tweets on date 2018-12-26 00:00:00\n",
      "Processing tweets on date 2018-12-27 00:00:00\n",
      "Processing tweets on date 2018-12-28 00:00:00\n",
      "Processing tweets on date 2018-12-29 00:00:00\n",
      "Processing tweets on date 2018-12-30 00:00:00\n",
      "Processing tweets on date 2018-12-31 00:00:00\n",
      "Processing tweets on date 2019-01-01 00:00:00\n",
      "Processing tweets on date 2019-01-02 00:00:00\n",
      "Processing tweets on date 2019-01-03 00:00:00\n",
      "Processing tweets on date 2019-01-04 00:00:00\n",
      "Processing tweets on date 2019-01-05 00:00:00\n",
      "Processing tweets on date 2019-01-06 00:00:00\n",
      "Processing tweets on date 2019-01-07 00:00:00\n",
      "Processing tweets on date 2019-01-08 00:00:00\n",
      "Processing tweets on date 2019-01-09 00:00:00\n",
      "Processing tweets on date 2019-01-10 00:00:00\n",
      "Processing tweets on date 2019-01-11 00:00:00\n",
      "Processing tweets on date 2019-01-12 00:00:00\n",
      "Processing tweets on date 2019-01-13 00:00:00\n",
      "Processing tweets on date 2019-01-14 00:00:00\n",
      "Processing tweets on date 2019-01-15 00:00:00\n",
      "Processing tweets on date 2019-01-16 00:00:00\n",
      "Processing tweets on date 2019-01-17 00:00:00\n",
      "Processing tweets on date 2019-01-18 00:00:00\n",
      "Processing tweets on date 2019-01-19 00:00:00\n",
      "Processing tweets on date 2019-01-20 00:00:00\n",
      "Processing tweets on date 2019-01-21 00:00:00\n",
      "Processing tweets on date 2019-01-22 00:00:00\n",
      "Processing tweets on date 2019-01-23 00:00:00\n",
      "Processing tweets on date 2019-01-24 00:00:00\n",
      "Processing tweets on date 2019-01-25 00:00:00\n",
      "Processing tweets on date 2019-01-26 00:00:00\n",
      "Processing tweets on date 2019-01-27 00:00:00\n",
      "Processing tweets on date 2019-01-28 00:00:00\n",
      "Processing tweets on date 2019-01-29 00:00:00\n",
      "Processing tweets on date 2019-01-30 00:00:00\n",
      "Processing tweets on date 2019-01-31 00:00:00\n",
      "Processing tweets on date 2019-02-01 00:00:00\n",
      "Processing tweets on date 2019-02-02 00:00:00\n",
      "Processing tweets on date 2019-02-03 00:00:00\n",
      "Processing tweets on date 2019-02-04 00:00:00\n",
      "Processing tweets on date 2019-02-05 00:00:00\n",
      "Processing tweets on date 2019-02-06 00:00:00\n",
      "Processing tweets on date 2019-02-07 00:00:00\n",
      "Processing tweets on date 2019-02-08 00:00:00\n",
      "Processing tweets on date 2019-02-09 00:00:00\n",
      "Processing tweets on date 2019-02-10 00:00:00\n",
      "Processing tweets on date 2019-02-11 00:00:00\n",
      "Processing tweets on date 2019-02-12 00:00:00\n",
      "Processing tweets on date 2019-02-13 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets on date 2019-02-14 00:00:00\n",
      "Processing tweets on date 2019-02-15 00:00:00\n",
      "Processing tweets on date 2019-02-16 00:00:00\n",
      "Processing tweets on date 2019-02-17 00:00:00\n",
      "Processing tweets on date 2019-02-18 00:00:00\n",
      "Processing tweets on date 2019-02-19 00:00:00\n",
      "Processing tweets on date 2019-02-20 00:00:00\n",
      "Processing tweets on date 2019-02-21 00:00:00\n",
      "Processing tweets on date 2019-02-22 00:00:00\n",
      "Processing tweets on date 2019-02-23 00:00:00\n",
      "Processing tweets on date 2019-02-24 00:00:00\n",
      "Processing tweets on date 2019-02-25 00:00:00\n",
      "Processing tweets on date 2019-02-26 00:00:00\n",
      "Processing tweets on date 2019-02-27 00:00:00\n",
      "Processing tweets on date 2019-02-28 00:00:00\n",
      "Processing tweets on date 2019-03-01 00:00:00\n",
      "Processing tweets on date 2019-03-02 00:00:00\n",
      "Processing tweets on date 2019-03-03 00:00:00\n",
      "Processing tweets on date 2019-03-04 00:00:00\n",
      "Processing tweets on date 2019-03-05 00:00:00\n",
      "Processing tweets on date 2019-03-06 00:00:00\n",
      "Processing tweets on date 2019-03-07 00:00:00\n",
      "Processing tweets on date 2019-03-08 00:00:00\n",
      "Processing tweets on date 2019-03-09 00:00:00\n",
      "Processing tweets on date 2019-03-10 00:00:00\n",
      "Processing tweets on date 2019-03-11 00:00:00\n",
      "Processing tweets on date 2019-03-12 00:00:00\n",
      "Processing tweets on date 2019-03-13 00:00:00\n",
      "Processing tweets on date 2019-03-14 00:00:00\n",
      "Processing tweets on date 2019-03-15 00:00:00\n",
      "Processing tweets on date 2019-03-16 00:00:00\n",
      "Processing tweets on date 2019-03-17 00:00:00\n",
      "Processing tweets on date 2019-03-18 00:00:00\n",
      "Processing tweets on date 2019-03-19 00:00:00\n",
      "Processing tweets on date 2019-03-20 00:00:00\n",
      "Processing tweets on date 2019-03-21 00:00:00\n",
      "Processing tweets on date 2019-03-22 00:00:00\n",
      "Processing tweets on date 2019-03-24 00:00:00\n",
      "Processing tweets on date 2019-03-25 00:00:00\n",
      "Processing tweets on date 2019-03-26 00:00:00\n",
      "Processing tweets on date 2019-03-27 00:00:00\n",
      "Processing tweets on date 2019-03-28 00:00:00\n",
      "Processing tweets on date 2019-03-29 00:00:00\n",
      "Processing tweets on date 2019-03-30 00:00:00\n",
      "Processing tweets on date 2019-03-31 00:00:00\n",
      "Processing tweets on date 2019-04-01 00:00:00\n",
      "Processing tweets on date 2019-04-02 00:00:00\n",
      "Processing tweets on date 2019-04-03 00:00:00\n",
      "Processing tweets on date 2019-04-04 00:00:00\n",
      "Processing tweets on date 2019-04-05 00:00:00\n",
      "Processing tweets on date 2019-04-06 00:00:00\n",
      "Processing tweets on date 2019-04-07 00:00:00\n",
      "Processing tweets on date 2019-04-08 00:00:00\n",
      "Processing tweets on date 2019-04-09 00:00:00\n",
      "Processing tweets on date 2019-04-10 00:00:00\n",
      "Processing tweets on date 2019-04-11 00:00:00\n",
      "Processing tweets on date 2019-04-12 00:00:00\n",
      "Processing tweets on date 2019-04-13 00:00:00\n",
      "Processing tweets on date 2019-04-14 00:00:00\n",
      "Processing tweets on date 2019-04-15 00:00:00\n",
      "Processing tweets on date 2019-04-16 00:00:00\n",
      "Processing tweets on date 2019-04-17 00:00:00\n",
      "Processing tweets on date 2019-04-18 00:00:00\n",
      "Processing tweets on date 2019-04-19 00:00:00\n",
      "Processing tweets on date 2019-04-20 00:00:00\n",
      "Processing tweets on date 2019-04-21 00:00:00\n",
      "Processing tweets on date 2019-04-22 00:00:00\n",
      "Processing tweets on date 2019-04-23 00:00:00\n",
      "Processing tweets on date 2019-04-24 00:00:00\n",
      "Processing tweets on date 2019-04-25 00:00:00\n",
      "Processing tweets on date 2019-04-26 00:00:00\n",
      "Processing tweets on date 2019-04-27 00:00:00\n",
      "Processing tweets on date 2019-04-28 00:00:00\n",
      "Processing tweets on date 2019-04-29 00:00:00\n",
      "Processing tweets on date 2019-04-30 00:00:00\n",
      "Processing tweets on date 2019-05-01 00:00:00\n",
      "Processing tweets on date 2019-05-02 00:00:00\n",
      "Processing tweets on date 2019-05-03 00:00:00\n",
      "Processing tweets on date 2019-05-04 00:00:00\n",
      "Processing tweets on date 2019-05-05 00:00:00\n",
      "Processing tweets on date 2019-05-06 00:00:00\n",
      "Processing tweets on date 2019-05-07 00:00:00\n",
      "Processing tweets on date 2019-05-08 00:00:00\n",
      "Processing tweets on date 2019-05-09 00:00:00\n",
      "Processing tweets on date 2019-05-10 00:00:00\n",
      "Processing tweets on date 2019-05-11 00:00:00\n",
      "Processing tweets on date 2019-05-12 00:00:00\n",
      "Processing tweets on date 2019-05-13 00:00:00\n",
      "Processing tweets on date 2019-05-14 00:00:00\n",
      "Processing tweets on date 2019-05-15 00:00:00\n",
      "Processing tweets on date 2019-05-16 00:00:00\n",
      "Processing tweets on date 2019-05-17 00:00:00\n",
      "Processing tweets on date 2019-05-18 00:00:00\n",
      "Processing tweets on date 2019-05-19 00:00:00\n",
      "Processing tweets on date 2019-05-20 00:00:00\n",
      "Processing tweets on date 2019-05-21 00:00:00\n",
      "Processing tweets on date 2019-05-22 00:00:00\n",
      "Processing tweets on date 2019-05-23 00:00:00\n",
      "Processing tweets on date 2019-05-24 00:00:00\n",
      "Processing tweets on date 2019-05-25 00:00:00\n",
      "Processing tweets on date 2019-05-26 00:00:00\n",
      "Processing tweets on date 2019-05-27 00:00:00\n",
      "Processing tweets on date 2019-05-28 00:00:00\n",
      "Processing tweets on date 2019-05-29 00:00:00\n",
      "Processing tweets on date 2019-05-30 00:00:00\n",
      "Processing tweets on date 2019-05-31 00:00:00\n",
      "Processing tweets on date 2019-06-01 00:00:00\n",
      "Processing tweets on date 2019-06-02 00:00:00\n",
      "Processing tweets on date 2019-06-03 00:00:00\n",
      "Processing tweets on date 2019-06-04 00:00:00\n",
      "Processing tweets on date 2019-06-05 00:00:00\n",
      "Processing tweets on date 2019-06-06 00:00:00\n",
      "Processing tweets on date 2019-06-07 00:00:00\n",
      "Processing tweets on date 2019-06-08 00:00:00\n",
      "Processing tweets on date 2019-06-09 00:00:00\n",
      "Processing tweets on date 2019-06-10 00:00:00\n",
      "Processing tweets on date 2019-06-11 00:00:00\n",
      "Processing tweets on date 2019-06-12 00:00:00\n",
      "Processing tweets on date 2019-06-13 00:00:00\n",
      "Processing tweets on date 2019-06-14 00:00:00\n",
      "Processing tweets on date 2019-06-15 00:00:00\n",
      "Processing tweets on date 2019-06-16 00:00:00\n",
      "Processing tweets on date 2019-06-17 00:00:00\n",
      "Processing tweets on date 2019-06-18 00:00:00\n",
      "Processing tweets on date 2019-06-19 00:00:00\n",
      "Processing tweets on date 2019-06-20 00:00:00\n",
      "Processing tweets on date 2019-06-21 00:00:00\n",
      "Processing tweets on date 2019-06-22 00:00:00\n",
      "Processing tweets on date 2019-06-23 00:00:00\n",
      "Processing tweets on date 2019-06-24 00:00:00\n",
      "Processing tweets on date 2019-06-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "file_list=[]\n",
    "Token=[]\n",
    "for d, twts in daily_tweets.iteritems():\n",
    "    text=' '.join(twts)  # Dirty document\n",
    "    token3=mypreprocess(text)\n",
    "    Token.append(token3)\n",
    "    new_text=' '.join(token3) # Clean document\n",
    "    file_list.append(new_text)\n",
    "    print(f'Processing tweets on date {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a data frame in python to store the main preprocessed text information. We have completed the preprocessing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.978Z"
    }
   },
   "outputs": [],
   "source": [
    "Date=daily_tweets.index\n",
    "# Whole string of cleaned text (untokenized)\n",
    "final=pd.DataFrame({'Date':Date,'Transformed Text':file_list})\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate corpus **document-term matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.980Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorization\n",
    "In the first example, we vectorize the corpus by the **raw count** of the preprocessed text. CountVectorizer is to set up the properties of the vectorization. fit_transform is to transform the data. If want to know more, go 'help(CountVectorize)' in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min_df` is a parameter to limit the number of features. If the document frequency of features is lower than 0.1 documents in the corpus, it won't be considered. `stop_words` option specifies the stop word list used in this model. In this case, we use the nltk default stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.988Z"
    }
   },
   "outputs": [],
   "source": [
    "tf=CountVectorizer(min_df=0.1,stop_words=stopwords.words('english'))\n",
    "tf1=tf.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we display the main attributes of the Vectorized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.991Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf1.shape);\n",
    "print(list(tf.get_feature_names())[0:20])   # The feature vectors\n",
    "print(list(tf.stop_words_)[0:20])  # Rare words that are droped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we used `tfidf` weighting to compute the document-term matrix. `tfidf` increases the significance (frequency) of terms that appear just in few documents, relative to those appearing in almost every document. Recall that\n",
    "\n",
    "$\\mathrm{tfidf(t, d)} = \\mathrm{tf}(t, d) \\times (\\log(\\frac{n}{\\mathrm{df(t)}})+1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.994Z"
    }
   },
   "outputs": [],
   "source": [
    "# max_df ignores terms with document frequency higher than the given threshold\n",
    "tfidf=TfidfVectorizer(min_df=0.1,max_df=0.9,stop_words=stopwords.words('english'))\n",
    "tfidf1=tfidf.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:05.996Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tfidf1.shape);\n",
    "print(list(tfidf.get_feature_names())[0:20])\n",
    "print(len(tfidf.get_feature_names()))\n",
    "print(list(tfidf.stop_words_)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the Document-Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.000Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf1.toarray())\n",
    "print(tfidf1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualized the popularity of terms in Trump's tweets. A naive way is to **aggregate the Document-Term matrix by columns.**\n",
    "\n",
    "Note that `numpy` provides an aggreagate-along-axis function, which should be much faster than homemade loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.004Z"
    }
   },
   "outputs": [],
   "source": [
    "Count = pd.DataFrame(data=dict(Term=tf.get_feature_names(),\n",
    "                               Count=np.sum(tf1.toarray(),\n",
    "                                            axis=0))).sort_values('Count', ascending=False)\n",
    "Count.reset_index(drop=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We played the same trick in the `tfidf` Document-Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.006Z"
    }
   },
   "outputs": [],
   "source": [
    "# CountTfidf = pd.DataFrame(data=dict(Term=tfidf.get_feature_names(),\n",
    "                               Count=np.sum(tfidf1.toarray(),\n",
    "                                            axis=0))).sort_values('Count', ascending=False)\n",
    "CountTfidf.reset_index(drop=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.009Z"
    }
   },
   "outputs": [],
   "source": [
    "topn=20\n",
    "Count.head(topn).set_index('Term').plot(kind='bar')\n",
    "plt.xticks(rotation=50)\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"Term Count\")\n",
    "plt.title('Simple Count');\n",
    "\n",
    "CountTfidf.head(topn).set_index('Term').plot(kind='bar')  # Already sorted\n",
    "plt.xticks(rotation=50)\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"Term Count\")\n",
    "plt.title('tfidf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "As demonstrated by the bar charts above, in both simple-count weighting and tfidf weighting, the most common word in Trump's tweets was \"great\". 'rt', abbreviation of 'retweet' ranked second, as Trump was fond of encouraging others to retweet his post. Other noticeable terms included 'border', 'fake' and 'news', highlighting his iconic project and feud with media. Somewhat amusingly, 'big' qualified as the 19th most frequent word. Trump's conversational, occasionally vulgar speaking style is not a novel subject.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "We utilized the Harvard-IV-Lasswell (`H4Lvd`) dictionary retrieved from [here](http://www.wjh.harvard.edu/~inquirer/homecat.htm). This is a dictionary augmented from Harvard-IV and Lasswell. Though there are a dozen of emotional categories in `H4Lvd`, we extracted only words from 'Positiv' and 'Negativ'.\n",
    "\n",
    "We chose this dictionary because of its comprehensiveness and authority.\n",
    "> The Harvard dictionary shown in the spreadsheet was expanded in 1998 to include almost all words [...] that occur 4 or more times per million according to the Thorndyke-Lorge counts.\n",
    "\n",
    "Compared to Loughran-McDonald dictionary, `H4Lvd` is more general-purpose rather than financially relavant. But since the president's tweets mostly do not refer to financials directly, a general dictionary fits our objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.013Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "DICT_PATH = 'inquireraugmented.xls'\n",
    "sentiment_dict = pd.read_excel(DICT_PATH, dtype=dict(Entry=str))\n",
    "# Select entries from Havard-IV & Lasswell\n",
    "sentiment_dict = sentiment_dict[sentiment_dict.Source == 'H4Lvd']\n",
    "sentiment_dict = sentiment_dict[['Entry', 'Positiv', 'Negativ']]\n",
    "sentiment_dict = sentiment_dict.dropna(how='all', subset=['Positiv', 'Negativ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.016Z"
    }
   },
   "outputs": [],
   "source": [
    "positive = sentiment_dict.Entry[~sentiment_dict.Positiv.isnull()]\n",
    "negative = sentiment_dict.Entry[~sentiment_dict.Negativ.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some entries, the augumented spreadsheet appends '#..' to the same word to distinguish meanings in different dictionaries. We took care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove hash sign\n",
    "positive = positive.map(lambda x: re.sub(r'#\\S+', '', x))\n",
    "negative = negative.map(lambda x: re.sub(r'#\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dictionary as well. This is becase their website don't provide dictionary in stemmed format. Usually, we need to be careful about this step, since not all documents demand this revision. The dictionary in R, for example, has already been preprocessed into stemmed format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.021Z"
    }
   },
   "outputs": [],
   "source": [
    "print(positive[:10])\n",
    "print(negative[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.023Z"
    }
   },
   "outputs": [],
   "source": [
    "#2. preprocess the dictionary\n",
    "p_list = positive.map(lambda x: PorterStemmer().stem(x.strip()).lower()).unique()\n",
    "n_list = negative.map(lambda x: PorterStemmer().stem(x.strip()).lower()).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can operate the text data with **dictionary method**. Here, we chooce two different ways to normalize the sentiment. One is by effective length of the document; the other is by the number of sentiment count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.026Z"
    }
   },
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.031Z"
    }
   },
   "outputs": [],
   "source": [
    "score1=[]\n",
    "score2=[]\n",
    "for text in final['Transformed Text']:\n",
    "    tokens = text.split(' ')\n",
    "    pos = [word for word in tokens if word in p_list]\n",
    "    neg = [word for word in tokens if word in n_list]\n",
    "    pos_count = len(pos)\n",
    "    neg_count = len(neg)\n",
    "    try:\n",
    "        score1.append((len(pos)-len(neg))/(len(pos)+len(neg)))  # Weighted by sentiment counts\n",
    "    except ZeroDivisionError:\n",
    "        score1.append(np.nan)\n",
    "    score2.append(((len(pos)-len(neg))/len(tokens)))  # Weighted by total document count\n",
    "\n",
    "x1 = list(daily_tweets.index)\n",
    "sentiment_Frame=pd.DataFrame({'Date':x1,'Score1':score1,'Score2':score2})\n",
    "sentiment_Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "We constructed two scores to quantify the emotional positivity on a daily basis. Both scores are simple count-based measure. The higher the score, the more 'positively' Trump expresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.034Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_Frame.set_index('Date').plot(style='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX index\n",
    "Trump has been famous for his obsessiveness to release disruptive information, opinion and decision on Twitter in a considerably real-time fashion. Those posts could have serious impact on investor confidence, business prospects and economic outlook. Ground-breaking posts such as those about US-China trade war might have agitated the market and geared up volatility. It comes natural to correlate the VIX \"fear index\" with his sentiment in tweets, trying to gauge if a more pessimistic Trump will stir investors' fear in a more uncertain economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.038Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "vix = pd.read_csv('VIX.csv',\n",
    "                  parse_dates=['Date'])[['Adj Close',\n",
    "                                         'Date']].rename(columns={'Adj Close': 'VIX'})\n",
    "df = pd.merge(vix, sentiment_Frame, on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to compare the trends is that the scale of the data could be completely different. To describle the trends, it's better to rescale the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.041Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.set_index('Date')\n",
    "df=df[['VIX','Score1', 'Score2']]\n",
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(data=scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.043Z"
    }
   },
   "outputs": [],
   "source": [
    "#3. Draw the graphs to show the sentiment trend\n",
    "x1=scaled_df.index\n",
    "y1=scaled_df['Score1']\n",
    "y2=scaled_df['Score2']\n",
    "y3=scaled_df['VIX']\n",
    "plt.plot(x1,y1,'b.',label='(Count-based) Score 1')\n",
    "plt.plot(x1,y2,'r.',label='(Count-based) Score 2')\n",
    "plt.plot(x1,y3,'c.',label='Standardized VIX')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute correlation\n",
    "scaled_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Significance test of the correlation coefficient\n",
    "def corr_p_value(r, N):\n",
    "    t = r * np.sqrt(N-2) / np.sqrt(1 - r**2)  # Test statistics, conforming to student-t distribution\n",
    "    return 2*t_dist.cdf(t, df=N-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.054Z"
    }
   },
   "outputs": [],
   "source": [
    "r = scaled_df.corr().loc['VIX', ['Score1', 'Score2']]\n",
    "p = corr_p_value(r, scaled_df.shape[0])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "The correlations between daily VIX levels and two simple count-based metrics are -7.5% and -5.3%. The corresponding two-tail p-values are 0.04771507 and 0.15407559. Hence, at 95% confidence level, the correlation is significance under Score 1 metric but not under Score 2 metric.\n",
    "\n",
    "The negative sign is what we expected: the more pessimistic and aggressive Trump gets, the more unsettled the market becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tfidf` term weighting\n",
    "- Sum up the `tfidf`-weighted frequencies of terms in two categories: 'Positiv' and 'Negativ'\n",
    "- Normalize the differential by document term count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.057Z"
    }
   },
   "outputs": [],
   "source": [
    "data = final.set_index('Date').loc[df.index]\n",
    "data['wordCount'] = data['Transformed Text'].map(lambda x: len(x.split(' ')))\n",
    "tfidf=TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "tfidf2=tfidf.fit_transform(list(data['Transformed Text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.059Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_mask = np.zeros(shape=len(tfidf.get_feature_names()))\n",
    "negative_mask = np.zeros(shape=len(tfidf.get_feature_names()))\n",
    "positive_bool = list(map(lambda x: x in p_list, tfidf.get_feature_names()))\n",
    "negative_bool = list(map(lambda x: x in n_list, tfidf.get_feature_names()))\n",
    "positive_mask[positive_bool] = 1\n",
    "negative_mask[negative_bool] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.062Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_values = np.dot(tfidf2.toarray(), positive_mask)\n",
    "negative_values = np.dot(tfidf2.toarray(), negative_mask)\n",
    "sentiment_scores = (positive_values - negative_values) / data.wordCount.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.065Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_summary = pd.merge(pd.DataFrame(dict(Date=df.index, score=sentiment_scores)),\n",
    "                         vix,\n",
    "                         on='Date').set_index('Date')\n",
    "tfidf_summary = pd.DataFrame(scaler.fit_transform(tfidf_summary.values),\n",
    "                             index=tfidf_summary.index,\n",
    "                             columns=tfidf_summary.columns)\n",
    "tfidf_summary.rename(columns=dict(VIX='Standardized VIX', score='tfidf Score'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.070Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_summary.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.072Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_summary.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-24T01:30:06.075Z"
    }
   },
   "outputs": [],
   "source": [
    "r = tfidf_summary.corr().iloc[0, 1]\n",
    "p = corr_p_value(r, tfidf_summary.shape[0])\n",
    "print('p-value = {:.4f}'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "Using the same setup, but with `tfidf` term weighting, the correlation increases to -8.2%, stilling passing the test at 95% C.I. The sign remains negative. Our conclusions do not change and we have stronger confidence in the significance of the relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
